{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c6742a51-fa60-4588-b032-26f87d07597c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataPreprocessing.ipynb  SepsisData.csv  Untitled1.ipynb  getting_started.ipynb\n",
      "FT_Sepsis.ipynb\t\t Untitled.ipynb  examples\n"
     ]
    }
   ],
   "source": [
    "!ls /teamspace/studios/this_studio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c8870413-c851-44b4-9a74-f10fb183cb96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nbconvert in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (7.16.4)\n",
      "Requirement already satisfied: beautifulsoup4 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nbconvert) (4.12.3)\n",
      "Requirement already satisfied: bleach!=5.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nbconvert) (6.2.0)\n",
      "Requirement already satisfied: defusedxml in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nbconvert) (0.7.1)\n",
      "Requirement already satisfied: jinja2>=3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nbconvert) (3.1.4)\n",
      "Requirement already satisfied: jupyter-core>=4.7 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nbconvert) (5.7.2)\n",
      "Requirement already satisfied: jupyterlab-pygments in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nbconvert) (0.3.0)\n",
      "Requirement already satisfied: markupsafe>=2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nbconvert) (3.0.2)\n",
      "Requirement already satisfied: mistune<4,>=2.0.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nbconvert) (3.0.2)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nbconvert) (0.10.2)\n",
      "Requirement already satisfied: nbformat>=5.7 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nbconvert) (5.10.4)\n",
      "Requirement already satisfied: packaging in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nbconvert) (24.2)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nbconvert) (1.5.1)\n",
      "Requirement already satisfied: pygments>=2.4.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nbconvert) (2.18.0)\n",
      "Requirement already satisfied: tinycss2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nbconvert) (1.4.0)\n",
      "Requirement already satisfied: traitlets>=5.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nbconvert) (5.14.3)\n",
      "Requirement already satisfied: webencodings in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from bleach!=5.0.0->nbconvert) (0.5.1)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jupyter-core>=4.7->nbconvert) (4.3.6)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nbclient>=0.5.0->nbconvert) (8.6.3)\n",
      "Requirement already satisfied: fastjsonschema>=2.15 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nbformat>=5.7->nbconvert) (2.21.1)\n",
      "Requirement already satisfied: jsonschema>=2.6 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nbformat>=5.7->nbconvert) (4.23.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from beautifulsoup4->nbconvert) (2.6)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert) (24.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert) (0.22.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert) (2.9.0.post0)\n",
      "Requirement already satisfied: pyzmq>=23.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert) (26.2.0)\n",
      "Requirement already satisfied: tornado>=6.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert) (6.4.2)\n",
      "Requirement already satisfied: six>=1.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from python-dateutil>=2.8.2->jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert) (1.17.0)\n",
      "[NbConvertApp] Converting notebook /teamspace/studios/this_studio/FT_Sepsis.ipynb to html\n",
      "[NbConvertApp] Writing 393774 bytes to /teamspace/studios/this_studio/FT_Sepsis.html\n"
     ]
    }
   ],
   "source": [
    "# # Install nbconvert if not already installed\n",
    "!pip install nbconvert\n",
    "\n",
    "# # Convert the current notebook to HTML\n",
    "!jupyter nbconvert --to html \"/teamspace/studios/this_studio/FT_Sepsis.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "107d1108-59f9-415b-9c91-6efd0ea7c3bc",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imblearn in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (0.0)\n",
      "Requirement already satisfied: imbalanced-learn in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from imblearn) (0.13.0)\n",
      "Requirement already satisfied: numpy<3,>=1.24.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from imbalanced-learn->imblearn) (1.26.4)\n",
      "Requirement already satisfied: scipy<2,>=1.10.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from imbalanced-learn->imblearn) (1.11.4)\n",
      "Requirement already satisfied: scikit-learn<2,>=1.3.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from imbalanced-learn->imblearn) (1.3.2)\n",
      "Requirement already satisfied: sklearn-compat<1,>=0.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from imbalanced-learn->imblearn) (0.1.3)\n",
      "Requirement already satisfied: joblib<2,>=1.1.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from imbalanced-learn->imblearn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl<4,>=2.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from imbalanced-learn->imblearn) (3.5.0)\n",
      "Requirement already satisfied: lib in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (4.0.0)\n",
      "Collecting torchsummary\n",
      "  Downloading torchsummary-1.5.1-py3-none-any.whl.metadata (296 bytes)\n",
      "Downloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\n",
      "Installing collected packages: torchsummary\n",
      "Successfully installed torchsummary-1.5.1\n"
     ]
    }
   ],
   "source": [
    "!pip install imblearn\n",
    "!pip install lib\n",
    "!pip install torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0688d0a1-524f-4c69-b127-88467373c0c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchinfo in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (1.8.0)\n",
      "Requirement already satisfied: pytorch-ignite in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (0.5.1)\n",
      "Requirement already satisfied: torch<3,>=1.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pytorch-ignite) (2.2.1+cu121)\n",
      "Requirement already satisfied: packaging in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pytorch-ignite) (24.2)\n",
      "Requirement already satisfied: filelock in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch<3,>=1.3->pytorch-ignite) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch<3,>=1.3->pytorch-ignite) (4.12.2)\n",
      "Requirement already satisfied: sympy in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch<3,>=1.3->pytorch-ignite) (1.13.3)\n",
      "Requirement already satisfied: networkx in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch<3,>=1.3->pytorch-ignite) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch<3,>=1.3->pytorch-ignite) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch<3,>=1.3->pytorch-ignite) (2024.12.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch<3,>=1.3->pytorch-ignite) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch<3,>=1.3->pytorch-ignite) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch<3,>=1.3->pytorch-ignite) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch<3,>=1.3->pytorch-ignite) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch<3,>=1.3->pytorch-ignite) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch<3,>=1.3->pytorch-ignite) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch<3,>=1.3->pytorch-ignite) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch<3,>=1.3->pytorch-ignite) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch<3,>=1.3->pytorch-ignite) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch<3,>=1.3->pytorch-ignite) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch<3,>=1.3->pytorch-ignite) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch<3,>=1.3->pytorch-ignite) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch<3,>=1.3->pytorch-ignite) (12.6.85)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jinja2->torch<3,>=1.3->pytorch-ignite) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sympy->torch<3,>=1.3->pytorch-ignite) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import math\n",
    "import warnings\n",
    "from typing import Dict, Literal\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.simplefilter(\"ignore\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from scipy.stats import zscore\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim\n",
    "from torch import Tensor\n",
    "from tqdm.std import tqdm\n",
    "warnings.resetwarnings()\n",
    "import lib\n",
    "import torchsummary\n",
    "!pip install torchinfo\n",
    "from torchinfo import summary\n",
    "!pip install pytorch-ignite\n",
    "from ignite.handlers import EarlyStopping\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5597cee2-4478-49ea-861f-8497a7663b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv('./SepsisData.csv', header=None, low_memory=False)\n",
    "\n",
    "# Step 1: Drop the first row (original header) and set a new header\n",
    "df = df.drop(index=0)\n",
    "df.columns = df.iloc[0]\n",
    "df = df.drop(index=1)\n",
    "\n",
    "# Define the columns based on the categories\n",
    "numeric_features = [\n",
    "    'age', 'BMI', 'gcs', 'sirs', 'apsiii', 'lods', 'oasis', 'sapsii', 'sofa_total',\n",
    "    'sofa_respiration', 'sofa_coagulation', 'sofa_liver', 'sofa_cardiovascular',\n",
    "    'sofa_cns', 'sofa_renal', 'urineoutput_1stday', 'hematocrit_min', 'hematocrit_max',\n",
    "    'hemoglobin_min', 'hemoglobin_max', 'platelets_min', 'platelets_max', 'wbc_min',\n",
    "    'wbc_max', 'albumin_min', 'albumin_max', 'aniongap_min', 'aniongap_max', 'bicarbonate_min',\n",
    "    'bicarbonate_max', 'calcium_min', 'calcium_max', 'chloride_min', 'chloride_max',\n",
    "    'glucose_mean', 'sodium_min', 'sodium_max', 'potassium_min', 'potassium_max', 'bun_max',\n",
    "    'creatinine_max', 'INR_min', 'INR_max', 'PT_min', 'PT_max', 'ptt_min', 'ptt_max',\n",
    "    'ALT_max', 'ALP_max', 'AST_max', 'bilirubin_total_max', 'ld_ldh_max', 'heart_rate_max',\n",
    "    'SBP_mean', 'DBP_mean', 'mbp_mean', 'resp_rate_min', 'resp_rate_max', 'temperature_min',\n",
    "    'temperature_max', 'SpO2_min', 'lactate_max_bg', 'pCO2_min_bg', 'pCO2_max_bg',\n",
    "    'baseexcess_min_bg', 'baseexcess_max_bg'\n",
    "]\n",
    "\n",
    "categorical_features = [\n",
    "    'gender_M1F0', 'Myocardial_infarction', 'Congestive_heart_failure', 'Peripheral_vascular_disease',\n",
    "    'Cerebrovascular_disease', 'Dementia', 'Chronic_pulmonary_disease', 'Rheumatic_disease',\n",
    "    'peptic_ulcer_disease', 'mild_liver_disease', 'Diabetes', 'Hemiplegia_paraplegia',\n",
    "    'renal_disease', 'malignancy', 'Moderate_or_severe_liver_disease', 'Metastatic_solid_tumor',\n",
    "    'AIDS', 'vasoactive drug ', 'dobutamine', 'vasopressin', 'phenylephrine', 'norepinephrine',\n",
    "    'dopamine', 'milrinone', 'epinephrine', 'MV'\n",
    "]\n",
    "\n",
    "label_encoding_feature = 'race'\n",
    "output_features = ['death_28day', 'death_90day', 'death_1year']\n",
    "\n",
    "# Step 2: Handle missing values\n",
    "# Fill numeric columns with mean\n",
    "for col in numeric_features:\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')  # Ensure numeric dtype\n",
    "    df[col] = df[col].fillna(df[col].mean())\n",
    "    df[col] = df[col].astype('int64')\n",
    "\n",
    "# Fill categorical columns with mode\n",
    "for col in categorical_features:\n",
    "    df[col] = df[col].fillna(df[col].mode()[0])\n",
    "    df[col] = df[col].astype('int64')\n",
    "\n",
    "# Step 3: Label encode the 'race' column\n",
    "label_encoder = LabelEncoder()\n",
    "df[label_encoding_feature] = label_encoder.fit_transform(df[label_encoding_feature].fillna(df[label_encoding_feature].mode()[0]))\n",
    "\n",
    "for col in output_features:\n",
    "    df[col] = df[col].astype('int64')\n",
    "\n",
    "# Step 4: Normalize all numeric columns using z-score\n",
    "for col in numeric_features:\n",
    "    df[col] = zscore(df[col])\n",
    "\n",
    "# Step 5: Extract X (features) and Y (target)\n",
    "Y = df['death_1year']  # Target column\n",
    "X = df.drop(columns=['death_1year', 'death_28day', 'death_90day','MV'])  # Features\n",
    "\n",
    "#Train-test split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42, stratify=Y)\n",
    "\n",
    "# Use RandomUnderSampler for imbalanced dataset\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "X_train_resampled, Y_train_resampled = rus.fit_resample(X_train, Y_train)\n",
    "categorical_features.pop(25)\n",
    "# print(\"X_train_resampled shape:\", X_train_resampled.shape)\n",
    "# print(\"Y_train_resampled shape:\", Y_train_resampled.shape)\n",
    "# Split training data into training and validation sets (e.g., 80% train, 20% val)\n",
    "X_train_resampled, X_val, Y_train_resampled, Y_val = train_test_split(\n",
    "    X_train_resampled, Y_train_resampled, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Convert the DataFrames to NumPy arrays before converting them to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train_resampled.to_numpy(), dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test.to_numpy(), dtype=torch.float32)\n",
    "Y_train_tensor = torch.tensor(Y_train_resampled.to_numpy(), dtype=torch.long)\n",
    "Y_test_tensor = torch.tensor(Y_test.to_numpy(), dtype=torch.long)\n",
    "\n",
    "X_val_tensor = torch.tensor(X_val.to_numpy(), dtype=torch.float32)\n",
    "Y_val_tensor = torch.tensor(Y_val.to_numpy(), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fcb66d3-31c6-45db-b585-9b157f1362f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Counts:\n",
      "death_1year\n",
      "0    14570\n",
      "1     4230\n",
      "Name: count, dtype: int64\n",
      "X_train_tensor shape: torch.Size([5414, 92])\n",
      "Y_train_tensor shape: torch.Size([5414])\n",
      "X_val_tensor shape: torch.Size([1354, 92])\n",
      "Y_val_tensor shape: torch.Size([1354])\n",
      "X_test_tensor shape: torch.Size([3760, 92])\n",
      "Y_test_tensor shape: torch.Size([3760])\n",
      "Count of each label after undersampling:\n",
      "death_1year\n",
      "1    2719\n",
      "0    2695\n",
      "Name: count, dtype: int64\n",
      "Count of each label in testset:\n",
      "0    2914\n",
      "1     846\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Count the number of rows with label 0 and label 1 in the training set\n",
    "label_counts = df['death_1year'].value_counts()\n",
    "print(\"Label Counts:\")\n",
    "print(label_counts)\n",
    "\n",
    "# print(\"X_train_resampled shape:\", X_train_resampled.shape)\n",
    "# print(\"Y_train_resampled shape:\", Y_train_resampled.shape)\n",
    "\n",
    "print(\"X_train_tensor shape:\", X_train_tensor.shape)\n",
    "print(\"Y_train_tensor shape:\", Y_train_tensor.shape)\n",
    "print(\"X_val_tensor shape:\", X_val_tensor.shape)\n",
    "print(\"Y_val_tensor shape:\", Y_val_tensor.shape)\n",
    "print(\"X_test_tensor shape:\", X_test_tensor.shape)\n",
    "print(\"Y_test_tensor shape:\", Y_test_tensor.shape)\n",
    "\n",
    "\n",
    "# Check the count of each label after undersampling\n",
    "label_counts = pd.Series(Y_train_resampled).value_counts()\n",
    "print(\"Count of each label after undersampling:\")\n",
    "print(label_counts)\n",
    "\n",
    "label_counts_testset = pd.Series(Y_test_tensor).value_counts()\n",
    "print(\"Count of each label in testset:\")\n",
    "print(label_counts_testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77bcfce5-720b-4f53-9067-92b521f12a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that all columns are encoded as needed and converted to PyTorch tensors\n",
    "\n",
    "# Separate numerical and categorical features\n",
    "X_train_num = torch.tensor(X_train_resampled[numeric_features].to_numpy(), dtype=torch.float32)\n",
    "X_train_cat = torch.tensor(X_train_resampled[categorical_features].to_numpy(), dtype=torch.long)\n",
    "\n",
    "X_test_num = torch.tensor(X_test[numeric_features].to_numpy(), dtype=torch.float32)\n",
    "X_test_cat = torch.tensor(X_test[categorical_features].to_numpy(), dtype=torch.long)\n",
    "\n",
    "# Target labels\n",
    "Y_train_tensor = torch.tensor(Y_train_resampled.to_numpy(), dtype=torch.long)\n",
    "Y_test_tensor = torch.tensor(Y_test.to_numpy(), dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1242dc8-2abe-44a6-aa9c-a6cc5a7fc179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.5826, -1.0166,  0.5710,  ..., -0.0359,  0.1061, -0.0194],\n",
      "        [ 0.2142,  0.0081,  0.2421,  ...,  1.5111,  0.5907,  2.0411],\n",
      "        [ 1.5650,  0.0081, -2.0604,  ..., -0.0359,  0.1061, -0.0194],\n",
      "        ...,\n",
      "        [ 0.5826,  0.6912,  0.2421,  ..., -0.0359,  0.1061, -0.0194],\n",
      "        [ 1.5650,  0.0081, -0.0869,  ..., -1.0369,  1.0753,  0.2750],\n",
      "        [-0.2770,  0.1789, -1.0736,  ...,  4.0592,  3.7408,  4.1016]])\n",
      "torch.Size([5414, 66])\n",
      "tensor([[1, 1, 0,  ..., 0, 0, 0],\n",
      "        [1, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 0, 1,  ..., 0, 0, 0],\n",
      "        [0, 0, 1,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])\n",
      "torch.Size([5414, 25])\n"
     ]
    }
   ],
   "source": [
    "print(X_train_num)\n",
    "print(X_train_num.shape)\n",
    "\n",
    "print(X_train_cat)\n",
    "print(X_train_cat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea4f66ee-6bc3-4913-8b11-9da23c5ac760",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "# %%\n",
    "import math\n",
    "import typing as ty\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as nn_init\n",
    "# import zero\n",
    "from torch import Tensor\n",
    "\n",
    "import lib\n",
    "\n",
    "\n",
    "# %%\n",
    "class Tokenizer(nn.Module):\n",
    "    category_offsets: ty.Optional[Tensor]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_numerical: int,       # Dimensionality of numerical features.\n",
    "        categories: ty.Optional[ty.List[int]],  # Cardinalities of categorical features.\n",
    "        d_token: int, # Dimensionality of token embeddings.\n",
    "        bias: bool,  # Whether to include a bias term.\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        if categories is None:\n",
    "            d_bias = d_numerical\n",
    "            self.category_offsets = None\n",
    "            self.category_embeddings = None\n",
    "        else:\n",
    "            d_bias = d_numerical + len(categories)\n",
    "            category_offsets = torch.tensor([0] + categories[:-1]).cumsum(0)\n",
    "            self.register_buffer('category_offsets', category_offsets)\n",
    "            self.category_embeddings = nn.Embedding(sum(categories), d_token)\n",
    "            nn_init.kaiming_uniform_(self.category_embeddings.weight, a=math.sqrt(5))\n",
    "            print(f'{self.category_embeddings.weight.shape=}')\n",
    "\n",
    "        # take [CLS] token into account\n",
    "        self.weight = nn.Parameter(Tensor(d_numerical + 1, d_token))  #Parameter matrix for embedding numerical features plus a [CLS] token\n",
    "        self.bias = nn.Parameter(Tensor(d_bias, d_token)) if bias else None #Optional bias matrix for adjusting the final embeddings\n",
    "        # The initialization is inspired by nn.Linear\n",
    "        nn_init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "        if self.bias is not None:\n",
    "            nn_init.kaiming_uniform_(self.bias, a=math.sqrt(5))\n",
    "\n",
    "    @property\n",
    "    def n_tokens(self) -> int:\n",
    "        return len(self.weight) + (\n",
    "            0 if self.category_offsets is None else len(self.category_offsets)\n",
    "        )\n",
    "\n",
    "    def forward(self, x_num: Tensor, x_cat: ty.Optional[Tensor]) -> Tensor:\n",
    "        x_some = x_num if x_cat is None else x_cat\n",
    "        assert x_some is not None\n",
    "        x_num = torch.cat(\n",
    "            [torch.ones(len(x_some), 1, device=x_some.device)]  # [CLS] Adds a [CLS] token (a column of ones) to x_num\n",
    "            + ([] if x_num is None else [x_num]),\n",
    "            dim=1,\n",
    "        )\n",
    "        x = self.weight[None] * x_num[:, :, None] #Computes embeddings for numerical features using self.weight\n",
    "        if x_cat is not None:\n",
    "            x = torch.cat(\n",
    "                [x, self.category_embeddings(x_cat + self.category_offsets[None])],\n",
    "                dim=1,\n",
    "            ) #Concatenates categorical and numerical embeddings along the feature dimension\n",
    "        if self.bias is not None:\n",
    "            bias = torch.cat(\n",
    "                [\n",
    "                    torch.zeros(1, self.bias.shape[1], device=x.device),\n",
    "                    self.bias,\n",
    "                ]\n",
    "            )\n",
    "            x = x + bias[None]\n",
    "        return x  #Returns the combined numerical and categorical token embeddings for the batch\n",
    "\n",
    "\n",
    "class MultiheadAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self, d: int, n_heads: int, dropout: float, initialization: str\n",
    "    ) -> None:\n",
    "        if n_heads > 1:\n",
    "            assert d % n_heads == 0\n",
    "        assert initialization in ['xavier', 'kaiming']\n",
    "\n",
    "        super().__init__()\n",
    "        self.W_q = nn.Linear(d, d)\n",
    "        self.W_k = nn.Linear(d, d)\n",
    "        self.W_v = nn.Linear(d, d)\n",
    "        self.W_out = nn.Linear(d, d) if n_heads > 1 else None\n",
    "        self.n_heads = n_heads\n",
    "        self.dropout = nn.Dropout(dropout) if dropout else None\n",
    "\n",
    "        for m in [self.W_q, self.W_k, self.W_v]:\n",
    "            if initialization == 'xavier' and (n_heads > 1 or m is not self.W_v):\n",
    "                # gain is needed since W_qkv is represented with 3 separate layers\n",
    "                nn_init.xavier_uniform_(m.weight, gain=1 / math.sqrt(2))\n",
    "            nn_init.zeros_(m.bias)\n",
    "        if self.W_out is not None:\n",
    "            nn_init.zeros_(self.W_out.bias)\n",
    "\n",
    "    def _reshape(self, x: Tensor) -> Tensor:\n",
    "        batch_size, n_tokens, d = x.shape\n",
    "        d_head = d // self.n_heads\n",
    "        return (\n",
    "            x.reshape(batch_size, n_tokens, self.n_heads, d_head)\n",
    "            .transpose(1, 2)\n",
    "            .reshape(batch_size * self.n_heads, n_tokens, d_head)\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x_q: Tensor,\n",
    "        x_kv: Tensor,\n",
    "        key_compression: ty.Optional[nn.Linear],\n",
    "        value_compression: ty.Optional[nn.Linear],\n",
    "    ) -> Tensor:\n",
    "        q, k, v = self.W_q(x_q), self.W_k(x_kv), self.W_v(x_kv)\n",
    "        for tensor in [q, k, v]:\n",
    "            assert tensor.shape[-1] % self.n_heads == 0\n",
    "        if key_compression is not None:\n",
    "            assert value_compression is not None\n",
    "            k = key_compression(k.transpose(1, 2)).transpose(1, 2)\n",
    "            v = value_compression(v.transpose(1, 2)).transpose(1, 2)\n",
    "        else:\n",
    "            assert value_compression is None\n",
    "\n",
    "        batch_size = len(q)\n",
    "        d_head_key = k.shape[-1] // self.n_heads\n",
    "        d_head_value = v.shape[-1] // self.n_heads\n",
    "        n_q_tokens = q.shape[1]\n",
    "\n",
    "        q = self._reshape(q)\n",
    "        k = self._reshape(k)\n",
    "        attention = F.softmax(q @ k.transpose(1, 2) / math.sqrt(d_head_key), dim=-1)\n",
    "        if self.dropout is not None:\n",
    "            attention = self.dropout(attention)\n",
    "        x = attention @ self._reshape(v)\n",
    "        x = (\n",
    "            x.reshape(batch_size, self.n_heads, n_q_tokens, d_head_value)\n",
    "            .transpose(1, 2)\n",
    "            .reshape(batch_size, n_q_tokens, self.n_heads * d_head_value)\n",
    "        )\n",
    "        if self.W_out is not None:\n",
    "            x = self.W_out(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    \"\"\"Transformer.\n",
    "\n",
    "    References:\n",
    "    - https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
    "    - https://github.com/facebookresearch/pytext/tree/master/pytext/models/representations/transformer\n",
    "    - https://github.com/pytorch/fairseq/blob/1bba712622b8ae4efb3eb793a8a40da386fe11d0/examples/linformer/linformer_src/modules/multihead_linear_attention.py#L19\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        # tokenizer\n",
    "        d_numerical: int,  # Number of numerical features.\n",
    "        categories: ty.Optional[ty.List[int]],  # Cardinalities of categorical features.\n",
    "        token_bias: bool,          # Whether to include bias in tokenizer embeddings.\n",
    "        # transformer\n",
    "        n_layers: int,             # Number of Transformer layers.\n",
    "        d_token: int,              # Dimensionality of token embeddings.\n",
    "        n_heads: int,              # Number of attention heads.\n",
    "        d_ffn_factor: float,       # Feedforward layer dimensionality as a multiplier of `d_token`.\n",
    "        attention_dropout: float,  # Dropout rate for attention.\n",
    "        ffn_dropout: float,        # Dropout rate for feedforward layers.\n",
    "        residual_dropout: float,   # Dropout rate for residual connections.\n",
    "        activation: str,           # Activation function name (e.g., \"relu\").\n",
    "        prenormalization: bool,    # Whether to apply layer normalization before operations.\n",
    "        initialization: str,       # Initialization method (\"xavier\" or others).\n",
    "        # linformer\n",
    "        kv_compression: ty.Optional[float],  # Compression ratio for attention keys/values.\n",
    "        kv_compression_sharing: ty.Optional[str],  # Sharing strategy for key/value compression.\n",
    "        d_out: int,                # Output dimensionality.\n",
    "    ) -> None:\n",
    "        assert (kv_compression is None) ^ (kv_compression_sharing is not None)\n",
    "\n",
    "        super().__init__()\n",
    "        self.tokenizer = Tokenizer(d_numerical, categories, d_token, token_bias)\n",
    "        n_tokens = self.tokenizer.n_tokens\n",
    "\n",
    "        def make_kv_compression():\n",
    "            assert kv_compression\n",
    "            compression = nn.Linear(\n",
    "                n_tokens, int(n_tokens * kv_compression), bias=False\n",
    "            )\n",
    "            if initialization == 'xavier':\n",
    "                nn_init.xavier_uniform_(compression.weight)\n",
    "            return compression\n",
    "\n",
    "        self.shared_kv_compression = (\n",
    "            make_kv_compression()\n",
    "            if kv_compression and kv_compression_sharing == 'layerwise'\n",
    "            else None\n",
    "        )\n",
    "\n",
    "        def make_normalization():\n",
    "            return nn.LayerNorm(d_token)\n",
    "\n",
    "        d_hidden = int(d_token * d_ffn_factor)\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for layer_idx in range(n_layers):\n",
    "            layer = nn.ModuleDict(\n",
    "                {\n",
    "                    'attention': MultiheadAttention(\n",
    "                        d_token, n_heads, attention_dropout, initialization\n",
    "                    ),\n",
    "                    'linear0': nn.Linear(\n",
    "                        d_token, d_hidden * (2 if activation.endswith('glu') else 1)\n",
    "                    ),\n",
    "                    'linear1': nn.Linear(d_hidden, d_token),\n",
    "                    'norm1': make_normalization(),\n",
    "                }\n",
    "            )\n",
    "            if not prenormalization or layer_idx:\n",
    "                layer['norm0'] = make_normalization()\n",
    "            if kv_compression and self.shared_kv_compression is None:\n",
    "                layer['key_compression'] = make_kv_compression()\n",
    "                if kv_compression_sharing == 'headwise':\n",
    "                    layer['value_compression'] = make_kv_compression()\n",
    "                else:\n",
    "                    assert kv_compression_sharing == 'key-value'\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        # Set activation functions directly in the Transformer class\n",
    "        self.activation = F.relu  # or any other activation function, like torch.nn.ReLU()\n",
    "        self.last_activation = F.relu  # or any other non-GLU activation\n",
    "        # self.activation = lib.get_activation_fn(activation)\n",
    "        # self.last_activation = lib.get_nonglu_activation_fn(activation)\n",
    "        self.prenormalization = prenormalization\n",
    "        self.last_normalization = make_normalization() if prenormalization else None\n",
    "        self.ffn_dropout = ffn_dropout\n",
    "        self.residual_dropout = residual_dropout\n",
    "        self.head = nn.Linear(d_token, d_out)\n",
    "\n",
    "    def _get_kv_compressions(self, layer):\n",
    "        return (\n",
    "            (self.shared_kv_compression, self.shared_kv_compression)\n",
    "            if self.shared_kv_compression is not None\n",
    "            else (layer['key_compression'], layer['value_compression'])\n",
    "            if 'key_compression' in layer and 'value_compression' in layer\n",
    "            else (layer['key_compression'], layer['key_compression'])\n",
    "            if 'key_compression' in layer\n",
    "            else (None, None)\n",
    "        )\n",
    "\n",
    "    def _start_residual(self, x, layer, norm_idx):\n",
    "        x_residual = x\n",
    "        if self.prenormalization:\n",
    "            norm_key = f'norm{norm_idx}'\n",
    "            if norm_key in layer:\n",
    "                x_residual = layer[norm_key](x_residual)\n",
    "        return x_residual\n",
    "\n",
    "    def _end_residual(self, x, x_residual, layer, norm_idx):\n",
    "        if self.residual_dropout:\n",
    "            x_residual = F.dropout(x_residual, self.residual_dropout, self.training)\n",
    "        x = x + x_residual\n",
    "        if not self.prenormalization:\n",
    "            x = layer[f'norm{norm_idx}'](x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x_num: Tensor, x_cat: ty.Optional[Tensor]) -> Tensor:\n",
    "        x = self.tokenizer(x_num, x_cat)\n",
    "\n",
    "        for layer_idx, layer in enumerate(self.layers):\n",
    "            is_last_layer = layer_idx + 1 == len(self.layers)\n",
    "            layer = ty.cast(ty.Dict[str, nn.Module], layer)\n",
    "\n",
    "            x_residual = self._start_residual(x, layer, 0)\n",
    "            x_residual = layer['attention'](\n",
    "                # for the last attention, it is enough to process only [CLS]\n",
    "                (x_residual[:, :1] if is_last_layer else x_residual),\n",
    "                x_residual,\n",
    "                *self._get_kv_compressions(layer),\n",
    "            )\n",
    "            if is_last_layer:\n",
    "                x = x[:, : x_residual.shape[1]]\n",
    "            x = self._end_residual(x, x_residual, layer, 0)\n",
    "\n",
    "            x_residual = self._start_residual(x, layer, 1)\n",
    "            x_residual = layer['linear0'](x_residual)\n",
    "            x_residual = self.activation(x_residual)\n",
    "            if self.ffn_dropout:\n",
    "                x_residual = F.dropout(x_residual, self.ffn_dropout, self.training)\n",
    "            x_residual = layer['linear1'](x_residual)\n",
    "            x = self._end_residual(x, x_residual, layer, 1)\n",
    "\n",
    "        assert x.shape[1] == 1  #x is a 3D tensor with shape [batch_size, seq_len, d_token]\n",
    "        x = x[:, 0] #The [CLS] token is always the first token in the sequence,extracts the embedding of the [CLS] token for each sample in the batch\n",
    "        if self.last_normalization is not None:\n",
    "            x = self.last_normalization(x)\n",
    "        x = self.last_activation(x)\n",
    "        x = self.head(x)\n",
    "        x = x.squeeze(-1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "352d976b-2b06-48f3-932f-088c064db3d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.category_embeddings.weight.shape=torch.Size([50, 64])\n"
     ]
    }
   ],
   "source": [
    "import lib\n",
    "# Define parameters for Model 1\n",
    "d_numerical = len(numeric_features)  # Number of numerical features\n",
    "categories = [X_train_resampled[cat].nunique() for cat in categorical_features]  # Number of unique values per categorical feature\n",
    "d_token = 64  # Embedding size for each token; adjust as needed\n",
    "n_heads = 4  # Number of attention heads\n",
    "n_layers = 3  # Number of transformer layers\n",
    "d_ffn_factor = 4.0  # Factor for feedforward network dimension\n",
    "attention_dropout = 0.1\n",
    "ffn_dropout = 0.1\n",
    "residual_dropout = 0.1\n",
    "activation = \"relu\"\n",
    "prenormalization = True\n",
    "initialization = \"xavier\"\n",
    "token_bias = True\n",
    "kv_compression = None  # or a float value for compression ratio\n",
    "kv_compression_sharing = None  # adjust based on kv_compression\n",
    "d_out = 1  # Number of output classes (e.g., for a classification problem)\n",
    "\n",
    "# Initialize model\n",
    "model = Transformer(\n",
    "    d_numerical=d_numerical,\n",
    "    categories=categories,\n",
    "    token_bias=token_bias,\n",
    "    n_layers=n_layers,\n",
    "    d_token=d_token,\n",
    "    n_heads=n_heads,\n",
    "    d_ffn_factor=d_ffn_factor,\n",
    "    attention_dropout=attention_dropout,\n",
    "    ffn_dropout=ffn_dropout,\n",
    "    residual_dropout=residual_dropout,\n",
    "    activation=activation,\n",
    "    prenormalization=prenormalization,\n",
    "    initialization=initialization,\n",
    "    kv_compression=kv_compression,\n",
    "    kv_compression_sharing=kv_compression_sharing,\n",
    "    d_out=d_out\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "699d9478-3677-4983-9222-7ff684a50f0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 0.7297\n",
      "Epoch [2/100], Loss: 0.7551\n",
      "Epoch [3/100], Loss: 0.7009\n",
      "Epoch [4/100], Loss: 0.6971\n",
      "Epoch [5/100], Loss: 0.7023\n",
      "Epoch [6/100], Loss: 0.7001\n",
      "Epoch [7/100], Loss: 0.6961\n",
      "Epoch [8/100], Loss: 0.6931\n",
      "Epoch [9/100], Loss: 0.6936\n",
      "Epoch [10/100], Loss: 0.6947\n",
      "Epoch [11/100], Loss: 0.6949\n",
      "Epoch [12/100], Loss: 0.6958\n",
      "Epoch [13/100], Loss: 0.6942\n",
      "Epoch [14/100], Loss: 0.6941\n",
      "Epoch [15/100], Loss: 0.6933\n",
      "Epoch [16/100], Loss: 0.6923\n",
      "Epoch [17/100], Loss: 0.6925\n",
      "Epoch [18/100], Loss: 0.6928\n",
      "Epoch [19/100], Loss: 0.6934\n",
      "Epoch [20/100], Loss: 0.6929\n",
      "Epoch [21/100], Loss: 0.6918\n",
      "Epoch [22/100], Loss: 0.6923\n",
      "Epoch [23/100], Loss: 0.6913\n",
      "Epoch [24/100], Loss: 0.6907\n",
      "Epoch [25/100], Loss: 0.6911\n",
      "Epoch [26/100], Loss: 0.6906\n",
      "Epoch [27/100], Loss: 0.6911\n",
      "Epoch [28/100], Loss: 0.6903\n",
      "Epoch [29/100], Loss: 0.6909\n",
      "Epoch [30/100], Loss: 0.6900\n",
      "Epoch [31/100], Loss: 0.6898\n",
      "Epoch [32/100], Loss: 0.6886\n",
      "Epoch [33/100], Loss: 0.6866\n",
      "Epoch [34/100], Loss: 0.6860\n",
      "Epoch [35/100], Loss: 0.6849\n",
      "Epoch [36/100], Loss: 0.6839\n",
      "Epoch [37/100], Loss: 0.6825\n",
      "Epoch [38/100], Loss: 0.6790\n",
      "Epoch [39/100], Loss: 0.6766\n",
      "Epoch [40/100], Loss: 0.6732\n",
      "Epoch [41/100], Loss: 0.6671\n",
      "Epoch [42/100], Loss: 0.6602\n",
      "Epoch [43/100], Loss: 0.6510\n",
      "Epoch [44/100], Loss: 0.6385\n",
      "Epoch [45/100], Loss: 0.6243\n",
      "Epoch [46/100], Loss: 0.6096\n",
      "Epoch [47/100], Loss: 0.5960\n",
      "Epoch [48/100], Loss: 0.5875\n",
      "Epoch [49/100], Loss: 0.5847\n",
      "Epoch [50/100], Loss: 0.5822\n",
      "Epoch [51/100], Loss: 0.5784\n",
      "Epoch [52/100], Loss: 0.5731\n",
      "Epoch [53/100], Loss: 0.5648\n",
      "Epoch [54/100], Loss: 0.5582\n",
      "Epoch [55/100], Loss: 0.5548\n",
      "Epoch [56/100], Loss: 0.5518\n",
      "Epoch [57/100], Loss: 0.5461\n",
      "Epoch [58/100], Loss: 0.5439\n",
      "Epoch [59/100], Loss: 0.5394\n",
      "Epoch [60/100], Loss: 0.5364\n",
      "Epoch [61/100], Loss: 0.5319\n",
      "Epoch [62/100], Loss: 0.5263\n",
      "Epoch [63/100], Loss: 0.5217\n",
      "Epoch [64/100], Loss: 0.5176\n",
      "Epoch [65/100], Loss: 0.5136\n",
      "Epoch [66/100], Loss: 0.5170\n",
      "Epoch [67/100], Loss: 0.5252\n",
      "Epoch [68/100], Loss: 0.5138\n",
      "Epoch [69/100], Loss: 0.5092\n",
      "Epoch [70/100], Loss: 0.5111\n",
      "Epoch [71/100], Loss: 0.5047\n",
      "Epoch [72/100], Loss: 0.5085\n",
      "Epoch [73/100], Loss: 0.5001\n",
      "Epoch [74/100], Loss: 0.5036\n",
      "Epoch [75/100], Loss: 0.4985\n",
      "Epoch [76/100], Loss: 0.5014\n",
      "Epoch [77/100], Loss: 0.5002\n",
      "Epoch [78/100], Loss: 0.4975\n",
      "Epoch [79/100], Loss: 0.4963\n",
      "Epoch [80/100], Loss: 0.4948\n",
      "Epoch [81/100], Loss: 0.4934\n",
      "Epoch [82/100], Loss: 0.4907\n",
      "Epoch [83/100], Loss: 0.4936\n",
      "Epoch [84/100], Loss: 0.4893\n",
      "Epoch [85/100], Loss: 0.4896\n",
      "Epoch [86/100], Loss: 0.4896\n",
      "Epoch [87/100], Loss: 0.4886\n",
      "Epoch [88/100], Loss: 0.4856\n",
      "Epoch [89/100], Loss: 0.4866\n",
      "Epoch [90/100], Loss: 0.4846\n",
      "Epoch [91/100], Loss: 0.4842\n",
      "Epoch [92/100], Loss: 0.4842\n",
      "Epoch [93/100], Loss: 0.4835\n",
      "Epoch [94/100], Loss: 0.4832\n",
      "Epoch [95/100], Loss: 0.4833\n",
      "Epoch [96/100], Loss: 0.4807\n",
      "Epoch [97/100], Loss: 0.4814\n",
      "Epoch [98/100], Loss: 0.4807\n",
      "Epoch [99/100], Loss: 0.4792\n",
      "Epoch [100/100], Loss: 0.4791\n"
     ]
    }
   ],
   "source": [
    "# Define loss and optimizer\n",
    "criterion = F.binary_cross_entropy_with_logits\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "n_epochs = 100\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    predictions = model(X_train_num, X_train_cat)\n",
    "\n",
    "    Y_train_tensor = Y_train_tensor.float()\n",
    "    # Compute loss\n",
    "    loss = criterion(predictions, Y_train_tensor)\n",
    "\n",
    "    # Backward pass and optimization\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{n_epochs}], Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78ff282c-e0b6-4a82-a2b0-30068b99aee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import torchsummary\n",
    "\n",
    "# Helper function to compute evaluation metrics\n",
    "def compute_metrics(y_true, y_pred):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, average=\"weighted\")\n",
    "    recall = recall_score(y_true, y_pred, average=\"weighted\")\n",
    "    f1 = f1_score(y_true, y_pred, average=\"weighted\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "# Display model summary\n",
    "def model_summary(model, input_size):\n",
    "    torchsummary.summary(model, input_size=input_size)\n",
    "\n",
    "# Plot training and validation loss\n",
    "def plot_loss(train_losses, val_losses=None):\n",
    "    plt.plot(train_losses, label=\"Train Loss\")\n",
    "    if val_losses:\n",
    "        plt.plot(val_losses, label=\"Validation Loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training and Validation Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d56f331-8de3-4cb1-8ffd-465e91aa23a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7266\n",
      "Precision: 0.8165\n",
      "Recall: 0.7266\n",
      "F1 Score: 0.7479\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.70      0.80      2914\n",
      "           1       0.44      0.80      0.57       846\n",
      "\n",
      "    accuracy                           0.73      3760\n",
      "   macro avg       0.68      0.75      0.68      3760\n",
      "weighted avg       0.82      0.73      0.75      3760\n",
      "\n"
     ]
    }
   ],
   "source": [
    "is_binary_classification = True  # Set to True if binary classification, False otherwise\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_predictions = model(X_test_num, X_test_cat)\n",
    "\n",
    "    if is_binary_classification:\n",
    "        # Binary classification\n",
    "        test_predictions = torch.sigmoid(test_predictions)\n",
    "        y_pred = (test_predictions > 0.5).long()  # Convert to binary\n",
    "    else:\n",
    "        # Multiclass classification\n",
    "        y_pred = torch.argmax(test_predictions, dim=1)  # Get predicted class indices\n",
    "\n",
    "# Now you can proceed with converting predictions and labels to CPU for metric calculations\n",
    "# Convert predictions and labels to CPU for metric calculations\n",
    "y_true = Y_test_tensor.cpu().numpy()\n",
    "y_pred = y_pred.cpu().numpy()\n",
    "\n",
    "# Compute metrics\n",
    "accuracy, precision, recall, f1 = compute_metrics(y_true, y_pred)\n",
    "\n",
    "# Display classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
