{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "27abd85a-4d60-4df4-9115-a145978841e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataPreprocessing.ipynb  SepsisData.csv\t\t       getting_started.ipynb\n",
      "FT_Sepsis.html\t\t Tab_transformer_Sepsis.ipynb\n",
      "FT_Sepsis.ipynb\t\t examples\n"
     ]
    }
   ],
   "source": [
    "!ls /teamspace/studios/this_studio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cc2f2c7a-e5f2-4692-86a7-ead06d801bc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nbconvert in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (7.16.4)\n",
      "Requirement already satisfied: beautifulsoup4 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nbconvert) (4.12.3)\n",
      "Requirement already satisfied: bleach!=5.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nbconvert) (6.2.0)\n",
      "Requirement already satisfied: defusedxml in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nbconvert) (0.7.1)\n",
      "Requirement already satisfied: jinja2>=3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nbconvert) (3.1.4)\n",
      "Requirement already satisfied: jupyter-core>=4.7 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nbconvert) (5.7.2)\n",
      "Requirement already satisfied: jupyterlab-pygments in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nbconvert) (0.3.0)\n",
      "Requirement already satisfied: markupsafe>=2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nbconvert) (3.0.2)\n",
      "Requirement already satisfied: mistune<4,>=2.0.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nbconvert) (3.0.2)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nbconvert) (0.10.2)\n",
      "Requirement already satisfied: nbformat>=5.7 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nbconvert) (5.10.4)\n",
      "Requirement already satisfied: packaging in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nbconvert) (24.2)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nbconvert) (1.5.1)\n",
      "Requirement already satisfied: pygments>=2.4.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nbconvert) (2.18.0)\n",
      "Requirement already satisfied: tinycss2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nbconvert) (1.4.0)\n",
      "Requirement already satisfied: traitlets>=5.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nbconvert) (5.14.3)\n",
      "Requirement already satisfied: webencodings in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from bleach!=5.0.0->nbconvert) (0.5.1)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jupyter-core>=4.7->nbconvert) (4.3.6)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nbclient>=0.5.0->nbconvert) (8.6.3)\n",
      "Requirement already satisfied: fastjsonschema>=2.15 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nbformat>=5.7->nbconvert) (2.21.1)\n",
      "Requirement already satisfied: jsonschema>=2.6 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nbformat>=5.7->nbconvert) (4.23.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from beautifulsoup4->nbconvert) (2.6)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert) (24.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert) (0.22.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert) (2.9.0.post0)\n",
      "Requirement already satisfied: pyzmq>=23.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert) (26.2.0)\n",
      "Requirement already satisfied: tornado>=6.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert) (6.4.2)\n",
      "Requirement already satisfied: six>=1.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from python-dateutil>=2.8.2->jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert) (1.17.0)\n",
      "[NbConvertApp] Converting notebook /teamspace/studios/this_studio/Tab_transformer_Sepsis.ipynb to html\n",
      "[NbConvertApp] Writing 379361 bytes to /teamspace/studios/this_studio/Tab_transformer_Sepsis.html\n"
     ]
    }
   ],
   "source": [
    "# # Install nbconvert if not already installed\n",
    "!pip install nbconvert\n",
    "\n",
    "# # Convert the current notebook to HTML\n",
    "!jupyter nbconvert --to html \"/teamspace/studios/this_studio/Tab_transformer_Sepsis.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "107d1108-59f9-415b-9c91-6efd0ea7c3bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imblearn in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (0.0)\n",
      "Requirement already satisfied: imbalanced-learn in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from imblearn) (0.13.0)\n",
      "Requirement already satisfied: numpy<3,>=1.24.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from imbalanced-learn->imblearn) (1.26.4)\n",
      "Requirement already satisfied: scipy<2,>=1.10.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from imbalanced-learn->imblearn) (1.11.4)\n",
      "Requirement already satisfied: scikit-learn<2,>=1.3.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from imbalanced-learn->imblearn) (1.3.2)\n",
      "Requirement already satisfied: sklearn-compat<1,>=0.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from imbalanced-learn->imblearn) (0.1.3)\n",
      "Requirement already satisfied: joblib<2,>=1.1.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from imbalanced-learn->imblearn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl<4,>=2.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from imbalanced-learn->imblearn) (3.5.0)\n",
      "Requirement already satisfied: lib in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (4.0.0)\n",
      "Requirement already satisfied: torchsummary in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (1.5.1)\n",
      "Requirement already satisfied: einops in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (0.8.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install imblearn\n",
    "!pip install lib\n",
    "!pip install torchsummary\n",
    "!pip install einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0688d0a1-524f-4c69-b127-88467373c0c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchinfo in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (1.8.0)\n",
      "Requirement already satisfied: pytorch-ignite in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (0.5.1)\n",
      "Requirement already satisfied: torch<3,>=1.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pytorch-ignite) (2.2.1+cu121)\n",
      "Requirement already satisfied: packaging in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pytorch-ignite) (24.2)\n",
      "Requirement already satisfied: filelock in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch<3,>=1.3->pytorch-ignite) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch<3,>=1.3->pytorch-ignite) (4.12.2)\n",
      "Requirement already satisfied: sympy in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch<3,>=1.3->pytorch-ignite) (1.13.3)\n",
      "Requirement already satisfied: networkx in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch<3,>=1.3->pytorch-ignite) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch<3,>=1.3->pytorch-ignite) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch<3,>=1.3->pytorch-ignite) (2024.12.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch<3,>=1.3->pytorch-ignite) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch<3,>=1.3->pytorch-ignite) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch<3,>=1.3->pytorch-ignite) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch<3,>=1.3->pytorch-ignite) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch<3,>=1.3->pytorch-ignite) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch<3,>=1.3->pytorch-ignite) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch<3,>=1.3->pytorch-ignite) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch<3,>=1.3->pytorch-ignite) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch<3,>=1.3->pytorch-ignite) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch<3,>=1.3->pytorch-ignite) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch<3,>=1.3->pytorch-ignite) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch<3,>=1.3->pytorch-ignite) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch<3,>=1.3->pytorch-ignite) (12.6.85)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jinja2->torch<3,>=1.3->pytorch-ignite) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sympy->torch<3,>=1.3->pytorch-ignite) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import math\n",
    "import warnings\n",
    "from typing import Dict, Literal\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.simplefilter(\"ignore\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from scipy.stats import zscore\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim\n",
    "from torch import Tensor\n",
    "from tqdm.std import tqdm\n",
    "warnings.resetwarnings()\n",
    "import lib\n",
    "import torchsummary\n",
    "!pip install torchinfo\n",
    "from torchinfo import summary\n",
    "!pip install pytorch-ignite\n",
    "from ignite.handlers import EarlyStopping\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5597cee2-4478-49ea-861f-8497a7663b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv('./SepsisData.csv', header=None, low_memory=False)\n",
    "\n",
    "# Step 1: Drop the first row (original header) and set a new header\n",
    "df = df.drop(index=0)\n",
    "df.columns = df.iloc[0]\n",
    "df = df.drop(index=1)\n",
    "\n",
    "# Define the columns based on the categories\n",
    "numeric_features = [\n",
    "    'age', 'BMI', 'gcs', 'sirs', 'apsiii', 'lods', 'oasis', 'sapsii', 'sofa_total',\n",
    "    'sofa_respiration', 'sofa_coagulation', 'sofa_liver', 'sofa_cardiovascular',\n",
    "    'sofa_cns', 'sofa_renal', 'urineoutput_1stday', 'hematocrit_min', 'hematocrit_max',\n",
    "    'hemoglobin_min', 'hemoglobin_max', 'platelets_min', 'platelets_max', 'wbc_min',\n",
    "    'wbc_max', 'albumin_min', 'albumin_max', 'aniongap_min', 'aniongap_max', 'bicarbonate_min',\n",
    "    'bicarbonate_max', 'calcium_min', 'calcium_max', 'chloride_min', 'chloride_max',\n",
    "    'glucose_mean', 'sodium_min', 'sodium_max', 'potassium_min', 'potassium_max', 'bun_max',\n",
    "    'creatinine_max', 'INR_min', 'INR_max', 'PT_min', 'PT_max', 'ptt_min', 'ptt_max',\n",
    "    'ALT_max', 'ALP_max', 'AST_max', 'bilirubin_total_max', 'ld_ldh_max', 'heart_rate_max',\n",
    "    'SBP_mean', 'DBP_mean', 'mbp_mean', 'resp_rate_min', 'resp_rate_max', 'temperature_min',\n",
    "    'temperature_max', 'SpO2_min', 'lactate_max_bg', 'pCO2_min_bg', 'pCO2_max_bg',\n",
    "    'baseexcess_min_bg', 'baseexcess_max_bg'\n",
    "]\n",
    "\n",
    "categorical_features = [\n",
    "    'gender_M1F0', 'Myocardial_infarction', 'Congestive_heart_failure', 'Peripheral_vascular_disease',\n",
    "    'Cerebrovascular_disease', 'Dementia', 'Chronic_pulmonary_disease', 'Rheumatic_disease',\n",
    "    'peptic_ulcer_disease', 'mild_liver_disease', 'Diabetes', 'Hemiplegia_paraplegia',\n",
    "    'renal_disease', 'malignancy', 'Moderate_or_severe_liver_disease', 'Metastatic_solid_tumor',\n",
    "    'AIDS', 'vasoactive drug ', 'dobutamine', 'vasopressin', 'phenylephrine', 'norepinephrine',\n",
    "    'dopamine', 'milrinone', 'epinephrine', 'MV'\n",
    "]\n",
    "\n",
    "label_encoding_feature = 'race'\n",
    "output_features = ['death_28day', 'death_90day', 'death_1year']\n",
    "\n",
    "# Step 2: Handle missing values\n",
    "# Fill numeric columns with mean\n",
    "for col in numeric_features:\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')  # Ensure numeric dtype\n",
    "    df[col] = df[col].fillna(df[col].mean())\n",
    "    df[col] = df[col].astype('int64')\n",
    "\n",
    "# Fill categorical columns with mode\n",
    "for col in categorical_features:\n",
    "    df[col] = df[col].fillna(df[col].mode()[0])\n",
    "    df[col] = df[col].astype('int64')\n",
    "\n",
    "# Step 3: Label encode the 'race' column\n",
    "label_encoder = LabelEncoder()\n",
    "df[label_encoding_feature] = label_encoder.fit_transform(df[label_encoding_feature].fillna(df[label_encoding_feature].mode()[0]))\n",
    "\n",
    "for col in output_features:\n",
    "    df[col] = df[col].astype('int64')\n",
    "\n",
    "# Step 4: Normalize all numeric columns using z-score\n",
    "for col in numeric_features:\n",
    "    df[col] = zscore(df[col])\n",
    "\n",
    "# Step 5: Extract X (features) and Y (target)\n",
    "Y = df['death_1year']  # Target column\n",
    "X = df.drop(columns=['death_1year', 'death_28day', 'death_90day','MV'])  # Features\n",
    "\n",
    "#Train-test split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42, stratify=Y)\n",
    "\n",
    "# Use RandomUnderSampler for imbalanced dataset\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "X_train_resampled, Y_train_resampled = rus.fit_resample(X_train, Y_train)\n",
    "categorical_features.pop(25)\n",
    "# print(\"X_train_resampled shape:\", X_train_resampled.shape)\n",
    "# print(\"Y_train_resampled shape:\", Y_train_resampled.shape)\n",
    "# Split training data into training and validation sets (e.g., 80% train, 20% val)\n",
    "X_train_resampled, X_val, Y_train_resampled, Y_val = train_test_split(\n",
    "    X_train_resampled, Y_train_resampled, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Convert the DataFrames to NumPy arrays before converting them to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train_resampled.to_numpy(), dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test.to_numpy(), dtype=torch.float32)\n",
    "Y_train_tensor = torch.tensor(Y_train_resampled.to_numpy(), dtype=torch.long)\n",
    "Y_test_tensor = torch.tensor(Y_test.to_numpy(), dtype=torch.long)\n",
    "\n",
    "X_val_tensor = torch.tensor(X_val.to_numpy(), dtype=torch.float32)\n",
    "Y_val_tensor = torch.tensor(Y_val.to_numpy(), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2fcb66d3-31c6-45db-b585-9b157f1362f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Counts:\n",
      "death_1year\n",
      "0    14570\n",
      "1     4230\n",
      "Name: count, dtype: int64\n",
      "X_train_tensor shape: torch.Size([5414, 92])\n",
      "Y_train_tensor shape: torch.Size([5414])\n",
      "X_val_tensor shape: torch.Size([1354, 92])\n",
      "Y_val_tensor shape: torch.Size([1354])\n",
      "X_test_tensor shape: torch.Size([3760, 92])\n",
      "Y_test_tensor shape: torch.Size([3760])\n",
      "Count of each label after undersampling:\n",
      "death_1year\n",
      "1    2719\n",
      "0    2695\n",
      "Name: count, dtype: int64\n",
      "Count of each label in testset:\n",
      "0    2914\n",
      "1     846\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Count the number of rows with label 0 and label 1 in the training set\n",
    "label_counts = df['death_1year'].value_counts()\n",
    "print(\"Label Counts:\")\n",
    "print(label_counts)\n",
    "\n",
    "# print(\"X_train_resampled shape:\", X_train_resampled.shape)\n",
    "# print(\"Y_train_resampled shape:\", Y_train_resampled.shape)\n",
    "\n",
    "print(\"X_train_tensor shape:\", X_train_tensor.shape)\n",
    "print(\"Y_train_tensor shape:\", Y_train_tensor.shape)\n",
    "print(\"X_val_tensor shape:\", X_val_tensor.shape)\n",
    "print(\"Y_val_tensor shape:\", Y_val_tensor.shape)\n",
    "print(\"X_test_tensor shape:\", X_test_tensor.shape)\n",
    "print(\"Y_test_tensor shape:\", Y_test_tensor.shape)\n",
    "\n",
    "\n",
    "# Check the count of each label after undersampling\n",
    "label_counts = pd.Series(Y_train_resampled).value_counts()\n",
    "print(\"Count of each label after undersampling:\")\n",
    "print(label_counts)\n",
    "\n",
    "label_counts_testset = pd.Series(Y_test_tensor).value_counts()\n",
    "print(\"Count of each label in testset:\")\n",
    "print(label_counts_testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77bcfce5-720b-4f53-9067-92b521f12a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that all columns are encoded as needed and converted to PyTorch tensors\n",
    "\n",
    "# Separate numerical and categorical features\n",
    "X_train_num = torch.tensor(X_train_resampled[numeric_features].to_numpy(), dtype=torch.float32)\n",
    "X_train_cat = torch.tensor(X_train_resampled[categorical_features].to_numpy(), dtype=torch.long)\n",
    "\n",
    "X_test_num = torch.tensor(X_test[numeric_features].to_numpy(), dtype=torch.float32)\n",
    "X_test_cat = torch.tensor(X_test[categorical_features].to_numpy(), dtype=torch.long)\n",
    "\n",
    "# Target labels\n",
    "Y_train_tensor = torch.tensor(Y_train_resampled.to_numpy(), dtype=torch.long)\n",
    "Y_test_tensor = torch.tensor(Y_test.to_numpy(), dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1239f506-bde1-4ee7-9bb3-fc6c5b1f247b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.5826, -1.0166,  0.5710,  ..., -0.0359,  0.1061, -0.0194],\n",
      "        [ 0.2142,  0.0081,  0.2421,  ...,  1.5111,  0.5907,  2.0411],\n",
      "        [ 1.5650,  0.0081, -2.0604,  ..., -0.0359,  0.1061, -0.0194],\n",
      "        ...,\n",
      "        [ 0.5826,  0.6912,  0.2421,  ..., -0.0359,  0.1061, -0.0194],\n",
      "        [ 1.5650,  0.0081, -0.0869,  ..., -1.0369,  1.0753,  0.2750],\n",
      "        [-0.2770,  0.1789, -1.0736,  ...,  4.0592,  3.7408,  4.1016]])\n",
      "torch.Size([5414, 66])\n",
      "tensor([[1, 1, 0,  ..., 0, 0, 0],\n",
      "        [1, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 0, 1,  ..., 0, 0, 0],\n",
      "        [0, 0, 1,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])\n",
      "torch.Size([5414, 25])\n"
     ]
    }
   ],
   "source": [
    "print(X_train_num)\n",
    "print(X_train_num.shape)\n",
    "\n",
    "print(X_train_cat)\n",
    "print(X_train_cat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ade424d-5e5f-4df2-800c-960c64ef9e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, einsum\n",
    "\n",
    "from einops import rearrange, repeat\n",
    "\n",
    "# helpers\n",
    "\n",
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "def default(val, d):\n",
    "    return val if exists(val) else d\n",
    "\n",
    "# classes\n",
    "\n",
    "class Residual(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(x, **kwargs) + x\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(self.norm(x), **kwargs)\n",
    "\n",
    "# attention\n",
    "\n",
    "class GEGLU(nn.Module):\n",
    "    def forward(self, x):\n",
    "        x, gates = x.chunk(2, dim = -1)\n",
    "        return x * F.gelu(gates)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, mult = 4, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, dim * mult * 2),\n",
    "            GEGLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(dim * mult, dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.net(x)\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        heads = 8,\n",
    "        dim_head = 16,\n",
    "        dropout = 0.\n",
    "    ):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head * heads\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
    "        self.to_out = nn.Linear(inner_dim, dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.heads\n",
    "        q, k, v = self.to_qkv(x).chunk(3, dim = -1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = h), (q, k, v))\n",
    "        sim = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n",
    "\n",
    "        attn = sim.softmax(dim = -1)\n",
    "        dropped_attn = self.dropout(attn)\n",
    "\n",
    "        out = einsum('b h i j, b h j d -> b h i d', dropped_attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)', h = h)\n",
    "        return self.to_out(out), attn\n",
    "\n",
    "# transformer\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        depth,\n",
    "        heads,\n",
    "        dim_head,\n",
    "        attn_dropout,\n",
    "        ff_dropout\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                PreNorm(dim, Attention(dim, heads = heads, dim_head = dim_head, dropout = attn_dropout)),\n",
    "                PreNorm(dim, FeedForward(dim, dropout = ff_dropout)),\n",
    "            ]))\n",
    "\n",
    "    def forward(self, x, return_attn = False):\n",
    "        post_softmax_attns = []\n",
    "\n",
    "        for attn, ff in self.layers:\n",
    "            attn_out, post_softmax_attn = attn(x)\n",
    "            post_softmax_attns.append(post_softmax_attn)\n",
    "\n",
    "            x = x + attn_out\n",
    "            x = ff(x) + x\n",
    "\n",
    "        if not return_attn:\n",
    "            return x\n",
    "\n",
    "        return x, torch.stack(post_softmax_attns)\n",
    "# mlp\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, dims, act = None):\n",
    "        super().__init__()\n",
    "        dims_pairs = list(zip(dims[:-1], dims[1:]))\n",
    "        layers = []\n",
    "        for ind, (dim_in, dim_out) in enumerate(dims_pairs):\n",
    "            is_last = ind >= (len(dims_pairs) - 1)\n",
    "            linear = nn.Linear(dim_in, dim_out)\n",
    "            layers.append(linear)\n",
    "\n",
    "            if is_last:\n",
    "                continue\n",
    "\n",
    "            act = default(act, nn.ReLU())\n",
    "            layers.append(act)\n",
    "\n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mlp(x)\n",
    "\n",
    "# main class\n",
    "\n",
    "class TabTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        categories,\n",
    "        num_continuous,\n",
    "        dim,\n",
    "        depth,\n",
    "        heads,\n",
    "        dim_head = 16,\n",
    "        dim_out = 1,\n",
    "        mlp_hidden_mults = (4, 2),\n",
    "        mlp_act = None,\n",
    "        num_special_tokens = 2,\n",
    "        continuous_mean_std = None,\n",
    "        attn_dropout = 0.,\n",
    "        ff_dropout = 0.,\n",
    "        use_shared_categ_embed = True,\n",
    "        shared_categ_dim_divisor = 8.   # in paper, they reserve dimension / 8 for category shared embedding\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert all(map(lambda n: n > 0, categories)), 'number of each category must be positive'\n",
    "        assert len(categories) + num_continuous > 0, 'input shape must not be null'\n",
    "\n",
    "        # categories related calculations\n",
    "\n",
    "        self.num_categories = len(categories)\n",
    "        self.num_unique_categories = sum(categories)\n",
    "\n",
    "        # create category embeddings table\n",
    "\n",
    "        self.num_special_tokens = num_special_tokens\n",
    "        total_tokens = self.num_unique_categories + num_special_tokens\n",
    "\n",
    "        shared_embed_dim = 0 if not use_shared_categ_embed else int(dim // shared_categ_dim_divisor)\n",
    "\n",
    "        self.category_embed = nn.Embedding(total_tokens, dim - shared_embed_dim)\n",
    "\n",
    "        # take care of shared category embed\n",
    "\n",
    "        self.use_shared_categ_embed = use_shared_categ_embed\n",
    "\n",
    "        if use_shared_categ_embed:\n",
    "            self.shared_category_embed = nn.Parameter(torch.zeros(self.num_categories, shared_embed_dim))\n",
    "            nn.init.normal_(self.shared_category_embed, std = 0.02)\n",
    "\n",
    "        # for automatically offsetting unique category ids to the correct position in the categories embedding table\n",
    "\n",
    "        if self.num_unique_categories > 0:\n",
    "            categories_offset = F.pad(torch.tensor(list(categories)), (1, 0), value = num_special_tokens)\n",
    "            categories_offset = categories_offset.cumsum(dim = -1)[:-1]\n",
    "            self.register_buffer('categories_offset', categories_offset)\n",
    "\n",
    "        # continuous\n",
    "\n",
    "        self.num_continuous = num_continuous\n",
    "\n",
    "        if self.num_continuous > 0:\n",
    "            if exists(continuous_mean_std):\n",
    "                assert continuous_mean_std.shape == (num_continuous, 2), f'continuous_mean_std must have a shape of ({num_continuous}, 2) where the last dimension contains the mean and variance respectively'\n",
    "            self.register_buffer('continuous_mean_std', continuous_mean_std)\n",
    "\n",
    "            self.norm = nn.LayerNorm(num_continuous)\n",
    "\n",
    "        # transformer\n",
    "\n",
    "        self.transformer = Transformer(\n",
    "            dim = dim,\n",
    "            depth = depth,\n",
    "            heads = heads,\n",
    "            dim_head = dim_head,\n",
    "            attn_dropout = attn_dropout,\n",
    "            ff_dropout = ff_dropout\n",
    "        )\n",
    "\n",
    "        # mlp to logits\n",
    "\n",
    "        input_size = (dim * self.num_categories) + num_continuous\n",
    "\n",
    "        hidden_dimensions = [input_size * t for t in  mlp_hidden_mults]\n",
    "        all_dimensions = [input_size, *hidden_dimensions, dim_out]\n",
    "\n",
    "        self.mlp = MLP(all_dimensions, act = mlp_act)\n",
    "\n",
    "    def forward(self, x_categ, x_cont, return_attn = False):\n",
    "        xs = []\n",
    "\n",
    "        assert x_categ.shape[-1] == self.num_categories, f'you must pass in {self.num_categories} values for your categories input'\n",
    "\n",
    "        if self.num_unique_categories > 0:\n",
    "            x_categ = x_categ + self.categories_offset\n",
    "\n",
    "            categ_embed = self.category_embed(x_categ)\n",
    "\n",
    "            if self.use_shared_categ_embed:\n",
    "                shared_categ_embed = repeat(self.shared_category_embed, 'n d -> b n d', b = categ_embed.shape[0])\n",
    "                categ_embed = torch.cat((categ_embed, shared_categ_embed), dim = -1)\n",
    "\n",
    "            x, attns = self.transformer(categ_embed, return_attn = True)\n",
    "\n",
    "            flat_categ = rearrange(x, 'b ... -> b (...)')\n",
    "            xs.append(flat_categ)\n",
    "\n",
    "        assert x_cont.shape[1] == self.num_continuous, f'you must pass in {self.num_continuous} values for your continuous input'\n",
    "\n",
    "        if self.num_continuous > 0:\n",
    "            if exists(self.continuous_mean_std):\n",
    "                mean, std = self.continuous_mean_std.unbind(dim = -1)\n",
    "                x_cont = (x_cont - mean) / std\n",
    "\n",
    "            normed_cont = self.norm(x_cont)\n",
    "            xs.append(normed_cont)\n",
    "\n",
    "        x = torch.cat(xs, dim = -1)\n",
    "        logits = self.mlp(x)\n",
    "\n",
    "        if not return_attn:\n",
    "            return logits\n",
    "\n",
    "        return logits, attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c1939dad-8326-4cd0-b58f-f4b905c4e46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of unique values for each categorical feature\n",
    "category_sizes = [X_train_cat[:, i].max().item() + 1 for i in range(X_train_cat.shape[1])]\n",
    "num_continuous = X_train_num.shape[1]  # 66 in this case (numeric_features)\n",
    "\n",
    "# Model parameters\n",
    "dim = 64  # embedding dimension, can be tuned\n",
    "depth = 4  # number of Transformer layers, can be tuned\n",
    "heads = 8  # number of attention heads, can be tuned\n",
    "dim_head = 16  # dimension per head, can be tuned\n",
    "dim_out = 2  # output dimension (e.g., binary classification with logits)\n",
    "mlp_hidden_mults = (4, 2)  # scaling for MLP hidden layers\n",
    "attn_dropout = 0.1\n",
    "ff_dropout = 0.1\n",
    "\n",
    "# Precomputed mean and std for continuous features (standardization)\n",
    "# continuous_mean_std = torch.tensor([X_train_num.mean(dim=0), X_train_num.std(dim=0)]).T\n",
    "# Precompute mean and std for continuous features (standardization)\n",
    "mean = X_train_num.mean(dim=0)\n",
    "std = X_train_num.std(dim=0)\n",
    "continuous_mean_std = torch.stack((mean, std), dim=1)  # Shape will be (num_continuous, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "95dcf4e7-9f5a-4bc8-a230-7ce43c540705",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TabTransformer(\n",
    "    categories=category_sizes,\n",
    "    num_continuous=num_continuous,\n",
    "    dim=dim,\n",
    "    depth=depth,\n",
    "    heads=heads,\n",
    "    dim_head=dim_head,\n",
    "    dim_out=dim_out,\n",
    "    mlp_hidden_mults=mlp_hidden_mults,\n",
    "    attn_dropout=attn_dropout,\n",
    "    ff_dropout=ff_dropout,\n",
    "    continuous_mean_std=continuous_mean_std\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d72a0643-ad74-4952-b39e-57d1b99fb502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 0.713356594303075\n",
      "Epoch 2/100, Loss: 0.5200731542180567\n",
      "Epoch 3/100, Loss: 0.5029106126112096\n",
      "Epoch 4/100, Loss: 0.4882618483375101\n",
      "Epoch 5/100, Loss: 0.4742837017073351\n",
      "Epoch 6/100, Loss: 0.45778900612803064\n",
      "Epoch 7/100, Loss: 0.4313568378196043\n",
      "Epoch 8/100, Loss: 0.4223144482163822\n",
      "Epoch 9/100, Loss: 0.40751823739093895\n",
      "Epoch 10/100, Loss: 0.3722338189973551\n",
      "Epoch 11/100, Loss: 0.3495246813577764\n",
      "Epoch 12/100, Loss: 0.3233927904902136\n",
      "Epoch 13/100, Loss: 0.3064892978571794\n",
      "Epoch 14/100, Loss: 0.2889961805742453\n",
      "Epoch 15/100, Loss: 0.28576364681562955\n",
      "Epoch 16/100, Loss: 0.24162552709526874\n",
      "Epoch 17/100, Loss: 0.22543686401537236\n",
      "Epoch 18/100, Loss: 0.2060361295719357\n",
      "Epoch 19/100, Loss: 0.1917062449871617\n",
      "Epoch 20/100, Loss: 0.17895610921413582\n",
      "Epoch 21/100, Loss: 0.16187063295613316\n",
      "Epoch 22/100, Loss: 0.1487095342236845\n",
      "Epoch 23/100, Loss: 0.11857827944845399\n",
      "Epoch 24/100, Loss: 0.09324323278577888\n",
      "Epoch 25/100, Loss: 0.10457175356226371\n",
      "Epoch 26/100, Loss: 0.08121231371838161\n",
      "Epoch 27/100, Loss: 0.07223839776809601\n",
      "Epoch 28/100, Loss: 0.07956002334351925\n",
      "Epoch 29/100, Loss: 0.07025054698700414\n",
      "Epoch 30/100, Loss: 0.07193609677479425\n",
      "Epoch 31/100, Loss: 0.05823555661139407\n",
      "Epoch 32/100, Loss: 0.03378609029853525\n",
      "Epoch 33/100, Loss: 0.056009856988470455\n",
      "Epoch 34/100, Loss: 0.0425167432794457\n",
      "Epoch 35/100, Loss: 0.033821601711508054\n",
      "Epoch 36/100, Loss: 0.05616060749350307\n",
      "Epoch 37/100, Loss: 0.056108864076668395\n",
      "Epoch 38/100, Loss: 0.048829622612238915\n",
      "Epoch 39/100, Loss: 0.04034747531593038\n",
      "Epoch 40/100, Loss: 0.04807417458265691\n",
      "Epoch 41/100, Loss: 0.046496503241873786\n",
      "Epoch 42/100, Loss: 0.02988365189615733\n",
      "Epoch 43/100, Loss: 0.031099971000581077\n",
      "Epoch 44/100, Loss: 0.032029482031580035\n",
      "Epoch 45/100, Loss: 0.026796636693196728\n",
      "Epoch 46/100, Loss: 0.0389835654057713\n",
      "Epoch 47/100, Loss: 0.04248453908914919\n",
      "Epoch 48/100, Loss: 0.03982703012314981\n",
      "Epoch 49/100, Loss: 0.03803031949195001\n",
      "Epoch 50/100, Loss: 0.03432084239707632\n",
      "Epoch 51/100, Loss: 0.034081536448887645\n",
      "Epoch 52/100, Loss: 0.03062453206156908\n",
      "Epoch 53/100, Loss: 0.04445877152239144\n",
      "Epoch 54/100, Loss: 0.0394336192869934\n",
      "Epoch 55/100, Loss: 0.02820240673091201\n",
      "Epoch 56/100, Loss: 0.04132753186713671\n",
      "Epoch 57/100, Loss: 0.03921588547450751\n",
      "Epoch 58/100, Loss: 0.009297730672441877\n",
      "Epoch 59/100, Loss: 0.026405550684541916\n",
      "Epoch 60/100, Loss: 0.026553575057572314\n",
      "Epoch 61/100, Loss: 0.033225478158085346\n",
      "Epoch 62/100, Loss: 0.022715339951633617\n",
      "Epoch 63/100, Loss: 0.04790510473541123\n",
      "Epoch 64/100, Loss: 0.0376722798736249\n",
      "Epoch 65/100, Loss: 0.027782525332654854\n",
      "Epoch 66/100, Loss: 0.02149177853420149\n",
      "Epoch 67/100, Loss: 0.00843978357651769\n",
      "Epoch 68/100, Loss: 0.02383507563646323\n",
      "Epoch 69/100, Loss: 0.03652396367962116\n",
      "Epoch 70/100, Loss: 0.03733354851136777\n",
      "Epoch 71/100, Loss: 0.026461214066298186\n",
      "Epoch 72/100, Loss: 0.015756446772042288\n",
      "Epoch 73/100, Loss: 0.02475634733845629\n",
      "Epoch 74/100, Loss: 0.01686119127003782\n",
      "Epoch 75/100, Loss: 0.02152093191388678\n",
      "Epoch 76/100, Loss: 0.011908144643200265\n",
      "Epoch 77/100, Loss: 0.029255231024409807\n",
      "Epoch 78/100, Loss: 0.036865116088346014\n",
      "Epoch 79/100, Loss: 0.04273838700497864\n",
      "Epoch 80/100, Loss: 0.03044669768256319\n",
      "Epoch 81/100, Loss: 0.04224171850097157\n",
      "Epoch 82/100, Loss: 0.020388594209664327\n",
      "Epoch 83/100, Loss: 0.021049312349724486\n",
      "Epoch 84/100, Loss: 0.008596540312586352\n",
      "Epoch 85/100, Loss: 0.010865063562189735\n",
      "Epoch 86/100, Loss: 0.009252708870409312\n",
      "Epoch 87/100, Loss: 0.024930258948173852\n",
      "Epoch 88/100, Loss: 0.031033291677426057\n",
      "Epoch 89/100, Loss: 0.019407711341852305\n",
      "Epoch 90/100, Loss: 0.014934879296459471\n",
      "Epoch 91/100, Loss: 0.015332969801919288\n",
      "Epoch 92/100, Loss: 0.015439452327449116\n",
      "Epoch 93/100, Loss: 0.03159213490647985\n",
      "Epoch 94/100, Loss: 0.048724140703650255\n",
      "Epoch 95/100, Loss: 0.0391616972272138\n",
      "Epoch 96/100, Loss: 0.05564820264582308\n",
      "Epoch 97/100, Loss: 0.039410296279860674\n",
      "Epoch 98/100, Loss: 0.008404093342159392\n",
      "Epoch 99/100, Loss: 0.013796742185667534\n",
      "Epoch 100/100, Loss: 0.014868086844057871\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()  # suitable for multi-class or binary classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100  # can be adjusted\n",
    "batch_size = 32  # adjust as needed\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    # Mini-batch training\n",
    "    for i in range(0, X_train_num.size(0), batch_size):\n",
    "        # Get batch\n",
    "        x_batch_cat = X_train_cat[i:i+batch_size]\n",
    "        x_batch_num = X_train_num[i:i+batch_size]\n",
    "        y_batch = Y_train_tensor[i:i+batch_size]\n",
    "\n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        logits = model(x_batch_cat, x_batch_num)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(logits, y_batch)\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss / (i // batch_size + 1)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "29320ee9-6eb5-4cdd-92e5-c5207c3f883f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 71.68%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Forward pass on test set\n",
    "    test_logits = model(X_test_cat, X_test_num)\n",
    "\n",
    "    # Get predictions\n",
    "    test_preds = torch.argmax(test_logits, dim=1)  # Assuming a classification task\n",
    "    accuracy = (test_preds == Y_test_tensor).float().mean()\n",
    "\n",
    "    print(f\"Test Accuracy: {accuracy.item() * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a756d28-1429-4ece-a8d2-b9f6ac8ee4fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Evaluation Metrics:\n",
      "Accuracy: 0.7168\n",
      "Precision: 0.7954\n",
      "Recall: 0.7168\n",
      "F1 Score: 0.7379\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.71      0.80      2914\n",
      "           1       0.43      0.74      0.54       846\n",
      "\n",
      "    accuracy                           0.72      3760\n",
      "   macro avg       0.66      0.72      0.67      3760\n",
      "weighted avg       0.80      0.72      0.74      3760\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "import torch\n",
    "\n",
    "def evaluate_model(model, X_test_num, X_test_cat, Y_test_tensor):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        # Forward pass - only get the logits\n",
    "        logits = model(X_test_cat, X_test_num, return_attn=False)  # Set return_attn to False\n",
    "\n",
    "        # Get predicted classes\n",
    "        _, predicted = torch.max(logits, dim=1)\n",
    "\n",
    "    # Move data to CPU and convert to numpy arrays if necessary\n",
    "    Y_test_np = Y_test_tensor.cpu().numpy()\n",
    "    predicted_np = predicted.cpu().numpy()\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(Y_test_np, predicted_np)\n",
    "    precision = precision_score(Y_test_np, predicted_np, average='weighted')\n",
    "    recall = recall_score(Y_test_np, predicted_np, average='weighted')\n",
    "    f1 = f1_score(Y_test_np, predicted_np, average='weighted')\n",
    "\n",
    "    # Display results\n",
    "    print(\"Model Evaluation Metrics:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(Y_test_np, predicted_np))\n",
    "\n",
    "# Call the evaluation function\n",
    "evaluate_model(model, X_test_num, X_test_cat, Y_test_tensor)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
